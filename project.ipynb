{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAE 666 Project\n",
    "Author: Vishnu Vijay\n",
    "\n",
    "Description: AAE 666 Project Files and Implementation\n",
    "\n",
    "Date: Created - 4/21/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   IMPORT: Public Library\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#   IMPORT: Personal Library\n",
    "from dubins2d_agent import Dubins2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  CLASS: EDMD Loss Class for State Network\n",
    "class BilinearEDMD_Loss(nn.Module):\n",
    "    def __init__(self, num_lifted, num_states, model, X, Xn, U, w1=0.5, w2=0.5):\n",
    "        super(BilinearEDMD_Loss, self).__init__()\n",
    "        \n",
    "        self.num_lifted = num_lifted\n",
    "        self.num_states = num_states\n",
    "        self.model = model\n",
    "\n",
    "        self.X = torch.t(torch.from_numpy(X).to(torch.float32))\n",
    "        self.Xn = torch.t(torch.from_numpy(Xn).to(torch.float32))\n",
    "        self.U = (torch.from_numpy(U).to(torch.float32))\n",
    "\n",
    "        self.num_data = np.prod(self.U.size())\n",
    "\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "\n",
    "        self.A = np.zeros((num_lifted, num_lifted))\n",
    "        self.B = np.zeros((num_lifted, num_lifted))\n",
    "        self.C = torch.ones((self.num_states, self.num_lifted))\n",
    "\n",
    "\n",
    "    def forward(self, model_output, X_states):\n",
    "        # State Reconstruction Loss\n",
    "        loss1 = torch.linalg.matrix_norm(self.C@model_output - X_states)\n",
    "        print(loss1)\n",
    "\n",
    "        # Prediction Loss for Zn = D*Z + B*Z*U\n",
    "        Z = torch.t(self.model(self.X))\n",
    "        Zn = torch.t(self.model(self.Xn))\n",
    "        D_cp = cp.Variable((self.num_lifted, self.num_lifted))\n",
    "        B_cp = cp.Variable((self.num_lifted, self.num_lifted))\n",
    "        err = 0\n",
    "\n",
    "        for i in range(self.X.shape[1]):\n",
    "            z = Z[:, i].detach().numpy()\n",
    "            zn = Zn[:, i].detach().numpy()\n",
    "            u = self.U[:, i].detach().numpy()\n",
    "\n",
    "            err += cp.norm(zn - (D_cp @ z + B_cp @ z * u))\n",
    "\n",
    "        objective = cp.Minimize( err )\n",
    "        problem = cp.Problem(objective, [])\n",
    "        problem.solve()\n",
    "        assert problem.status == cp.OPTIMAL\n",
    "        print(\"D: \\n\", D_cp.value)\n",
    "        print(\"B: \\n\", B_cp.value)\n",
    "        self.D = torch.from_numpy(D_cp.value).to(torch.float32)\n",
    "        self.B = torch.from_numpy(B_cp.value).to(torch.float32)\n",
    "        \n",
    "        loss2 = torch.linalg.matrix_norm(Zn - self.D @ Z - self.B @ Z * self.U)\n",
    "\n",
    "        # Total Loss\n",
    "        self.total_loss = (self.w1*loss1 + self.w2*loss2)\n",
    "        self.avg_loss = torch.div(self.total_loss, self.num_data)\n",
    "        return self.total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  CLASS: Custom Deep Neural Network\n",
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden_layers):\n",
    "        super(CustomNN, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, 128)\n",
    "        self.hidden_layers = [] * num_hidden_layers\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            self.hidden_layers[i] = nn.Linear(128, 128)\n",
    "        self.output_layer = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            x = F.relu(layer(x))\n",
    "        x = F.relu(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   INITIALIZE: Simulation Parameters\n",
    "n = 3\n",
    "m = 1\n",
    "dt = 0.2\n",
    "sim_len = 2000\n",
    "traj_num = 1000\n",
    "num_data = sim_len * traj_num\n",
    "\n",
    "\n",
    "#   INITIALIZE: Bilinear EDMD Parameter\n",
    "num_lifted = 6\n",
    "hidden_layers = 2\n",
    "hidden_nodes = 128\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   INITIALIZE: Random Walk Car Controller\n",
    "def random_walk(x, u):\n",
    "    du = np.random.randint(low=-1, high=2)\n",
    "    u += du\n",
    "    return u\n",
    "speed = 1\n",
    "\n",
    "\n",
    "#   SIMULATE: Generate Trajectories for System Identification\n",
    "X = np.zeros((n, sim_len*traj_num))\n",
    "Xn = np.zeros((n, sim_len*traj_num))\n",
    "U = np.zeros((1, sim_len*traj_num))\n",
    "\n",
    "step = 0\n",
    "for i in range(traj_num):\n",
    "    theta0 = np.pi * (np.random.random() - 0.5)\n",
    "    y0 = 1_000 * (np.random.random() - 0.5)\n",
    "    z0 = 1_000 * (np.random.random() - 0.5)\n",
    "    x = np.array([[y0, z0, theta0]]).T\n",
    "\n",
    "    car_rand = Dubins2D(0, speed, x, random_walk, dt)\n",
    "    for i in range(sim_len):\n",
    "        X[:, step] = x.flatten()\n",
    "        x = car_rand.iterate_single()\n",
    "        U[:, step] = car_rand.input\n",
    "        Xn[:, step] = x.flatten()\n",
    "        step += 1\n",
    "\n",
    "\n",
    "#   SPLIT: Split the data into training and testing data\n",
    "training_split = round(num_data * 0.8)\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "# Xn_scaled = scaler.fit_transform(Xn)\n",
    "\n",
    "X_train = X[:, 0:training_split]\n",
    "Xn_train = Xn[:, 0:training_split]\n",
    "U_train = U[:, 0:training_split]\n",
    "\n",
    "X_test = X[:, training_split:]\n",
    "Xn_test = Xn[:, training_split:]\n",
    "U_test = U[:, training_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   IDENTIFY: System ID with DMDc\n",
    "# A = None\n",
    "# B = None\n",
    "# A_cp = cp.Variable((n, n))\n",
    "# B_cp = cp.Variable((n, m))\n",
    "# err = 0\n",
    "# for i in range(traj_num*sim_len):\n",
    "#     x = X[:, i]\n",
    "#     xn = Xn[:, i]\n",
    "#     u = U[:, i]\n",
    "#     err += cp.norm(xn - (A_cp @ x + B_cp @ u))\n",
    "# objective = cp.Minimize( err )\n",
    "# problem = cp.Problem(objective, [])\n",
    "# problem.solve()\n",
    "# if problem.status == cp.OPTIMAL:\n",
    "#     print(\"SOLVED\")\n",
    "#     A = A_cp.value\n",
    "#     B = B_cp.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(Z, Zn, U, criterion):\n",
    "    # Extract Criterion variables\n",
    "    D = criterion.D\n",
    "    B = criterion.B\n",
    "    w1 = criterion.w1\n",
    "    w2 = criterion.w2\n",
    "\n",
    "    C = np.zeros((n, num_lifted))\n",
    "\n",
    "    # State Reconstruction Loss\n",
    "    loss1 = np.linalg.norm(C@Z - X)\n",
    "\n",
    "    # Prediction Loss for Zn = D*Z + B*Z*U\n",
    "    loss2 = np.linalg.norm(Zn - D @ Z - B @ Z * U)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = w1*loss1 + w2*loss2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4613123.5000, grad_fn=<LinalgVectorNormBackward0>)\n",
      "D: \n",
      " [[-4.95839165 -0.         -0.         -0.          3.85785498  2.71034332]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-4.46988201 -0.         -0.         -0.          3.89430491  2.03283886]\n",
      " [ 3.71653945 -0.         -0.         -0.         -2.40480426 -0.6922566 ]]\n",
      "B: \n",
      " [[-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]]\n",
      "tensor(7.0124e+14, grad_fn=<LinalgVectorNormBackward0>)\n",
      "D: \n",
      " [[-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -0.00000000e+00 -0.00000000e+00]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "   9.99494804e-01  5.02940830e-04]\n",
      " [-0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  -2.62540886e-06  9.99967148e-01]]\n",
      "B: \n",
      " [[-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]]\n",
      "tensor(2.7622e+13, grad_fn=<LinalgVectorNormBackward0>)\n",
      "D: \n",
      " [[-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.          1.00026599]]\n",
      "B: \n",
      " [[-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]\n",
      " [-0. -0. -0. -0. -0. -0.]]\n",
      "tensor(4364769., grad_fn=<LinalgVectorNormBackward0>)\n",
      "D: \n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "B: \n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "tensor(4364769., grad_fn=<LinalgVectorNormBackward0>)\n",
      "D: \n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "B: \n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 5/5, Train Loss: 2182384.5000, Train Accuracy: 0.00, Test Accuracy: 0.00\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#   IDENTIFY: System ID step with EDMD with DNN\n",
    "w1 = 0.5\n",
    "w2 = 0.5\n",
    "learning_rate = 1000\n",
    "\n",
    "model = CustomNN(n, num_lifted, hidden_layers)\n",
    "criterion = BilinearEDMD_Loss(num_lifted, n, model, X_train, Xn_train, U_train, w1, w2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss=[]\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "X_train_tens = torch.t(torch.from_numpy(X_train).to(torch.float32))\n",
    "Xn_train_tens = torch.t(torch.from_numpy(Xn_train).to(torch.float32))\n",
    "X_test_tens = torch.t(torch.from_numpy(X_test).to(torch.float32))\n",
    "Xn_test_tens = torch.t(torch.from_numpy(Xn_test).to(torch.float32))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #forward feed\n",
    "    Z_train = torch.t(model(X_train_tens))\n",
    "    Zn_train = torch.t(model(Xn_train_tens))\n",
    "    \n",
    "    this_train_accuracy = 0 # get_accuracy(Z_train, Zn_train, U_train, criterion)\n",
    "    train_accuracy.append(this_train_accuracy)\n",
    "\n",
    "    #calculate the loss\n",
    "    loss = criterion(Z_train, torch.t(X_train_tens))\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    #clear out the gradients from the last step loss.backward()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #backward propagation: calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Z_test = torch.t(model(X_test_tens))\n",
    "        Zn_test = torch.t(model(Xn_test_tens))\n",
    "        this_test_accuracy = 0 # get_accuracy(Z_test, Zn_test, U_train, criterion.D, criterion.B)\n",
    "        test_accuracy.append(this_test_accuracy)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.4f},\",\n",
    "                f\"Train Accuracy: {sum(train_accuracy)/len(train_accuracy):.2f},\", \n",
    "                f\"Test Accuracy: {sum(test_accuracy)/len(test_accuracy):.2f}\")\n",
    "\n",
    "print(criterion.D)\n",
    "print(criterion.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   INITIALIZE: Reference Trajectory\n",
    "ref = lambda k : np.array([[-k*np.sin(3*k/100)],\n",
    "                           [-k*np.cos(3*k/100)],\n",
    "                           [3*k/100]])\n",
    "\n",
    "#   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
